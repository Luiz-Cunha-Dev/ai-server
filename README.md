# ğŸ¤– Server AI

API simples usando Node.js, Express e Ollama para chat com modelos de IA localmente.

## âœ¨ CaracterÃ­sticas

- **ğŸš€ API Express**: Endpoints para chat normal e streaming
- **ğŸ§  Ollama**: ExecuÃ§Ã£o local de modelos LLM
- **ğŸ³ Docker Compose**: Setup completo com containers
- **ğŸŒ CORS**: RequisiÃ§Ãµes cross-origin habilitadas
- **â³ Auto-wait**: Aguarda Ollama estar disponÃ­vel antes de iniciar

## ğŸ“¦ Estrutura

```
server-ai/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ index.js           # API Express
â”œâ”€â”€ docker-compose.yml     # Containers
â”œâ”€â”€ Dockerfile            # Imagem Node.js
â”œâ”€â”€ package.json          # DependÃªncias
â””â”€â”€ README.md            # Este arquivo
```

## ğŸ› ï¸ Tecnologias

- **Node.js 18+** + **Express** + **Ollama**
- **Docker & Docker Compose**
- **CORS** + **dotenv**

## âš¡ Como usar

### 1. Criar arquivo de ambiente

Crie `.env` na raiz:

```env
PORT=3000
MODEL=gemma2:2b
OLLAMA_HOST=http://ollama:11434
```

### 2. Iniciar

```powershell
docker-compose up -d
```

### 3. Testar

```powershell
# Chat normal
$body = @{ message = "OlÃ¡!" } | ConvertTo-Json
Invoke-RestMethod -Uri "http://localhost:3000/chat" -Method POST -Body $body -ContentType "application/json"

# Chat streaming
Invoke-RestMethod -Uri "http://localhost:3000/chat/stream" -Method POST -Body $body -ContentType "application/json"
```

## ğŸ”Œ API

### POST /chat
Chat normal - retorna resposta completa

### POST /chat/stream  
Chat streaming - retorna resposta em tempo real via Server-Sent Events

**Request:**
```json
{
  "message": "Sua pergunta"
}
```

## ğŸ”§ Comandos Ãºteis

```powershell
# Ver status
docker-compose ps

# Ver logs
docker-compose logs -f

# Modelos instalados
docker exec ollama-server ollama list

# Baixar modelo
docker exec ollama-server ollama pull gemma2:2b
```

## ğŸš¨ Problemas comuns

### Container nÃ£o inicia
```powershell
docker-compose down --remove-orphans
docker-compose build --no-cache
docker-compose up -d
```

### Modelo nÃ£o responde
```powershell
# Verificar se modelo estÃ¡ baixado
docker exec ollama-server ollama list

# Baixar manualmente
docker exec ollama-server ollama pull gemma2:2b
```

## ğŸ“ LicenÃ§a

MIT License