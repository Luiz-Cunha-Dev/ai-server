# Server AI

Um ambiente completo de desenvolvimento de IA local usando Docker containers com Ollama para executar modelos LLM e uma API Node.js personalizada para interaÃ§Ã£o com os modelos.

## ğŸš€ CaracterÃ­sticas

- **API Node.js/Express**: Servidor customizado para interaÃ§Ã£o com modelos AI
- **Ollama Server**: Container para execuÃ§Ã£o de modelos LLM localmente
- **Docker Compose**: OrquestraÃ§Ã£o completa dos serviÃ§os
- **Modelo Gemma2:2b**: Modelo prÃ©-configurado para chat
- **CORS habilitado**: Permite requisiÃ§Ãµes de diferentes origens
- **Aguarda conexÃ£o**: Sistema inteligente de espera pela disponibilidade do Ollama

## ğŸ“¦ Estrutura do Projeto

```
server-ai/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ index.js           # API Express principal
â”œâ”€â”€ docker-compose.yml     # OrquestraÃ§Ã£o dos containers
â”œâ”€â”€ Dockerfile            # Imagem do servidor Node.js
â”œâ”€â”€ package.json          # DependÃªncias e scripts
â”œâ”€â”€ .env                  # VariÃ¡veis de ambiente
â”œâ”€â”€ .gitignore           # Arquivos ignorados pelo Git
â””â”€â”€ README.md            # Este arquivo
```

## ğŸ› ï¸ Tecnologias

- **Node.js** com Express
- **Ollama** para modelos LLM
- **Docker & Docker Compose**
- **CORS** para requisiÃ§Ãµes cross-origin
- **dotenv** para variÃ¡veis de ambiente

## âš¡ InÃ­cio RÃ¡pido

### PrÃ©-requisitos
- Docker e Docker Compose instalados
- Git (para clonar o repositÃ³rio)

### 1. Clonar e configurar

```bash
git clone <url-do-repositÃ³rio>
cd server-ai
```

### 2. Criar arquivo de ambiente

Crie um arquivo `.env` na raiz do projeto:

```env
PORT=3000
MODEL=gemma2:2b
OLLAMA_HOST=http://ollama:11434
```

### 3. Iniciar os serviÃ§os

```bash
docker-compose up -d
```

### 4. Aguardar inicializaÃ§Ã£o

O sistema aguardarÃ¡ automaticamente o Ollama estar disponÃ­vel e baixarÃ¡ o modelo se necessÃ¡rio.

### 5. Testar a API

```bash
curl -X POST http://localhost:3000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "OlÃ¡, como vocÃª estÃ¡?"}'
```

## ğŸ”Œ API Endpoints

### POST /chat

Envia uma mensagem para o modelo AI e retorna a resposta.

**Request:**
```json
{
  "message": "Sua pergunta aqui"
}
```

**Response:**
```json
{
  "response": "Resposta do modelo AI"
}
```

**Exemplo com curl:**
```bash
curl -X POST http://localhost:3000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Explique o que Ã© inteligÃªncia artificial"}'
```

**Exemplo com JavaScript:**
```javascript
const response = await fetch('http://localhost:3000/chat', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    message: 'Como funciona o machine learning?'
  })
});

const data = await response.json();
console.log(data.response);
```

## ğŸ³ ServiÃ§os Docker

### Ollama Server
- **Porta**: 11434
- **Container**: `ollama-server`
- **Volume**: `ollama_data` para persistÃªncia dos modelos
- **Modelo padrÃ£o**: gemma2:2b

### Server AI (Node.js)
- **Porta**: 3000
- **Container**: `server-ai`
- **Build**: Dockerfile personalizado
- **Depende**: ollama service

## ğŸ”§ Comandos Ãšteis

### Gerenciar containers
```bash
# Ver status dos containers
docker-compose ps

# Ver logs
docker-compose logs

# Ver logs em tempo real
docker-compose logs -f

# Parar serviÃ§os
docker-compose down

# Reiniciar serviÃ§os
docker-compose restart
```

### Gerenciar modelos Ollama
```bash
# Listar modelos instalados
docker exec ollama-server ollama list

# Baixar novo modelo
docker exec ollama-server ollama pull <nome-do-modelo>

# Testar modelo diretamente
docker exec -it ollama-server ollama run gemma2:2b "Sua pergunta"

# Remover modelo
docker exec ollama-server ollama rm <nome-do-modelo>
```

### Desenvolvimento local
```bash
# Instalar dependÃªncias
npm install

# Executar em modo desenvolvimento
npm run dev

# Executar em produÃ§Ã£o
npm start
```

## ğŸŒ VariÃ¡veis de Ambiente

| VariÃ¡vel | DescriÃ§Ã£o | PadrÃ£o |
|----------|-----------|---------|
| `PORT` | Porta do servidor Express | `3000` |
| `MODEL` | Modelo Ollama a usar | `gemma2:2b` |
| `OLLAMA_HOST` | URL do servidor Ollama | `http://ollama:11434` |

## ğŸ“Š Monitoramento

### Health Checks
```bash
# Status da API
curl http://localhost:3000/health

# Status do Ollama
curl http://localhost:11434/api/tags
```

### Logs e Debug
```bash
# Logs do servidor Node.js
docker-compose logs server-ai

# Logs do Ollama
docker-compose logs ollama

# Entrar no container para debug
docker exec -it server-ai /bin/bash
```

## ğŸ”„ Modelos DisponÃ­veis

Alguns modelos populares que vocÃª pode usar:

- `gemma2:2b` - Modelo leve e rÃ¡pido (padrÃ£o)
- `llama3.1:8b` - Modelo mais poderoso
- `codellama:7b` - Especializado em cÃ³digo
- `mistral:7b` - Boa performance geral

Para trocar de modelo, atualize a variÃ¡vel `MODEL` no `.env` e reinicie os containers.

## ğŸš¨ SoluÃ§Ã£o de Problemas

### Container nÃ£o inicia
```bash
# Verificar logs
docker-compose logs

# Reconstruir imagens
docker-compose build --no-cache
```

### Modelo nÃ£o responde
```bash
# Verificar se o modelo estÃ¡ baixado
docker exec ollama-server ollama list

# Baixar modelo manualmente
docker exec ollama-server ollama pull gemma2:2b
```

### Erro de conexÃ£o
- Verifique se o Ollama estÃ¡ rodando: `docker-compose ps`
- Verifique a variÃ¡vel `OLLAMA_HOST` no `.env`
- Aguarde alguns segundos para inicializaÃ§Ã£o completa

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ licenciado sob a MIT License - veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ¯ PrÃ³ximos Passos

- [ ] Adicionar autenticaÃ§Ã£o JWT
- [ ] Implementar rate limiting
- [ ] Adicionar suporte a mÃºltiplos modelos
- [ ] Interface web para chat
- [ ] MÃ©tricas e monitoring
- [ ] Testes automatizados
- [ ] Deploy para produÃ§Ã£o

---

**Desenvolvido com â¤ï¸ usando Node.js, Express e Ollama**